{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d14928ee",
   "metadata": {},
   "source": [
    "# EndoMamba é¡¹ç›®ç¯å¢ƒé…ç½®å’Œä¾èµ–å®‰è£…æŒ‡å—\n",
    "\n",
    "æœ¬ç¬”è®°æœ¬æä¾›äº†åœ¨äº‘æœåŠ¡å™¨ä¸Šè®¾ç½® EndoMamba é¡¹ç›®å®Œæ•´ç¯å¢ƒçš„è¯¦ç»†æ­¥éª¤ï¼ŒåŒ…æ‹¬ï¼š\n",
    "\n",
    "ğŸ“‹ **é…ç½®è¦æ±‚**\n",
    "- Python 3.9\n",
    "- CUDA 12.4  \n",
    "- PyTorch 2.4.1+cu121\n",
    "- å„ç§æ·±åº¦å­¦ä¹ å’Œè®¡ç®—æœºè§†è§‰åº“\n",
    "- Mamba SSM å’Œ causal-conv1d è‡ªå®šä¹‰æ„å»º\n",
    "\n",
    "âš¡ **ä¸»è¦åŠŸèƒ½**\n",
    "1. ğŸ” ç¯å¢ƒæ£€æµ‹å’ŒéªŒè¯\n",
    "2. ğŸ“¦ ä¾èµ–åŒ…å®‰è£…\n",
    "3. ğŸ”¨ æºä»£ç æ„å»º\n",
    "4. âœ… åŠŸèƒ½éªŒè¯æµ‹è¯•\n",
    "\n",
    "ğŸ’¡ **æç¤º**: è¯·æŒ‰é¡ºåºæ‰§è¡Œæ¯ä¸ªæ­¥éª¤ï¼Œç¡®ä¿æ¯ä¸ªé˜¶æ®µéƒ½æˆåŠŸå®Œæˆåå†è¿›è¡Œä¸‹ä¸€æ­¥ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de701d5c",
   "metadata": {},
   "source": [
    "## 1. äº‘æœåŠ¡å™¨ç¯å¢ƒæ£€æµ‹ ğŸ”\n",
    "\n",
    "åœ¨å¼€å§‹å®‰è£…ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦æ£€æµ‹å½“å‰ç³»ç»Ÿç¯å¢ƒï¼ŒåŒ…æ‹¬æ“ä½œç³»ç»Ÿã€Pythonç‰ˆæœ¬ã€CUDAç‰ˆæœ¬ã€PyTorchç‰ˆæœ¬ï¼Œä»¥åŠç¡¬ä»¶é…ç½®ä¿¡æ¯ï¼ˆGPUã€CPUã€å†…å­˜ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75539fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ–¥ï¸  ç³»ç»ŸåŸºæœ¬ä¿¡æ¯\n",
      "============================================================\n",
      "æ“ä½œç³»ç»Ÿ: Linux 5.15.0-112-generic\n",
      "æ¶æ„: x86_64\n",
      "Python ç‰ˆæœ¬: 3.9.19 (main, May  6 2024, 19:43:03) \n",
      "[GCC 11.2.0]\n",
      "Python å¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„: /root/miniconda/bin/python\n",
      "\n",
      "============================================================\n",
      "ğŸ”§ CPU ä¿¡æ¯\n",
      "============================================================\n",
      "CPU æ ¸å¿ƒæ•°: 36 ç‰©ç†æ ¸å¿ƒ\n",
      "CPU çº¿ç¨‹æ•°: 72 é€»è¾‘æ ¸å¿ƒ\n",
      "CPU é¢‘ç‡: 1379.17 MHz (æœ€å¤§: 3700.00 MHz)\n",
      "\n",
      "============================================================\n",
      "ğŸ’¾ å†…å­˜ä¿¡æ¯\n",
      "============================================================\n",
      "æ€»å†…å­˜: 251.33 GB\n",
      "å¯ç”¨å†…å­˜: 241.47 GB\n",
      "å·²ä½¿ç”¨å†…å­˜: 8.08 GB\n",
      "å†…å­˜ä½¿ç”¨ç‡: 3.9%\n"
     ]
    }
   ],
   "source": [
    "# æ£€æµ‹ç³»ç»ŸåŸºæœ¬ä¿¡æ¯\n",
    "import sys\n",
    "import platform\n",
    "import os\n",
    "import subprocess\n",
    "import psutil\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ–¥ï¸  ç³»ç»ŸåŸºæœ¬ä¿¡æ¯\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"æ“ä½œç³»ç»Ÿ: {platform.system()} {platform.release()}\")\n",
    "print(f\"æ¶æ„: {platform.machine()}\")\n",
    "print(f\"Python ç‰ˆæœ¬: {sys.version}\")\n",
    "print(f\"Python å¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„: {sys.executable}\")\n",
    "\n",
    "# æ£€æµ‹CPUä¿¡æ¯\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ”§ CPU ä¿¡æ¯\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"CPU æ ¸å¿ƒæ•°: {psutil.cpu_count(logical=False)} ç‰©ç†æ ¸å¿ƒ\")\n",
    "print(f\"CPU çº¿ç¨‹æ•°: {psutil.cpu_count(logical=True)} é€»è¾‘æ ¸å¿ƒ\")\n",
    "cpu_freq = psutil.cpu_freq()\n",
    "if cpu_freq:\n",
    "    print(f\"CPU é¢‘ç‡: {cpu_freq.current:.2f} MHz (æœ€å¤§: {cpu_freq.max:.2f} MHz)\")\n",
    "\n",
    "# æ£€æµ‹å†…å­˜ä¿¡æ¯\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ’¾ å†…å­˜ä¿¡æ¯\")\n",
    "print(\"=\" * 60)\n",
    "memory = psutil.virtual_memory()\n",
    "print(f\"æ€»å†…å­˜: {memory.total / (1024**3):.2f} GB\")\n",
    "print(f\"å¯ç”¨å†…å­˜: {memory.available / (1024**3):.2f} GB\")\n",
    "print(f\"å·²ä½¿ç”¨å†…å­˜: {memory.used / (1024**3):.2f} GB\")\n",
    "print(f\"å†…å­˜ä½¿ç”¨ç‡: {memory.percent:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50d14db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ® GPU å’Œ CUDA ä¿¡æ¯\n",
      "============================================================\n",
      "âœ… NVIDIA GPU æ£€æµ‹æˆåŠŸ:\n",
      "NVIDIA-SMI ç‰ˆæœ¬: | NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "\n",
      "âœ… PyTorch å·²å®‰è£…: 2.4.1+cu121\n",
      "CUDA å¯ç”¨: âœ… æ˜¯\n",
      "CUDA ç‰ˆæœ¬ (PyTorch): 12.1\n",
      "cuDNN ç‰ˆæœ¬: 90100\n",
      "GPU æ•°é‡: 1\n",
      "GPU 0: NVIDIA GeForce RTX 3090 (23.7 GB)\n",
      "\n",
      "âœ… CUDA Toolkit å·²å®‰è£…:\n",
      "CUDA Toolkit ç‰ˆæœ¬: Cuda compilation tools, release 12.4, V12.4.131\n"
     ]
    }
   ],
   "source": [
    "# æ£€æµ‹CUDAå’ŒGPUä¿¡æ¯\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ® GPU å’Œ CUDA ä¿¡æ¯\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# æ£€æµ‹NVIDIA GPU\n",
    "try:\n",
    "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, check=True)\n",
    "    print(\"âœ… NVIDIA GPU æ£€æµ‹æˆåŠŸ:\")\n",
    "    lines = result.stdout.split('\\n')\n",
    "    for line in lines:\n",
    "        if 'NVIDIA-SMI' in line:\n",
    "            print(f\"NVIDIA-SMI ç‰ˆæœ¬: {line.strip()}\")\n",
    "        elif 'CUDA Version' in line:\n",
    "            cuda_version = line.split('CUDA Version: ')[1].split()[0] if 'CUDA Version: ' in line else \"æœªçŸ¥\"\n",
    "            print(f\"CUDA é©±åŠ¨ç‰ˆæœ¬: {cuda_version}\")\n",
    "except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "    print(\"âŒ æœªæ£€æµ‹åˆ° NVIDIA GPU æˆ– nvidia-smi æœªå®‰è£…\")\n",
    "\n",
    "# æ£€æµ‹PyTorchä¸­çš„CUDAæ”¯æŒ\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"\\nâœ… PyTorch å·²å®‰è£…: {torch.__version__}\")\n",
    "    print(f\"CUDA å¯ç”¨: {'âœ… æ˜¯' if torch.cuda.is_available() else 'âŒ å¦'}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA ç‰ˆæœ¬ (PyTorch): {torch.version.cuda}\")\n",
    "        print(f\"cuDNN ç‰ˆæœ¬: {torch.backends.cudnn.version()}\")\n",
    "        print(f\"GPU æ•°é‡: {torch.cuda.device_count()}\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            gpu_name = torch.cuda.get_device_name(i)\n",
    "            gpu_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n",
    "            print(f\"GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
    "except ImportError:\n",
    "    print(\"âŒ PyTorch æœªå®‰è£…\")\n",
    "\n",
    "# æ£€æµ‹CUDA Toolkit\n",
    "try:\n",
    "    result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True, check=True)\n",
    "    print(f\"\\nâœ… CUDA Toolkit å·²å®‰è£…:\")\n",
    "    for line in result.stdout.split('\\n'):\n",
    "        if 'release' in line:\n",
    "            print(f\"CUDA Toolkit ç‰ˆæœ¬: {line.strip()}\")\n",
    "            break\n",
    "except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "    print(\"\\nâŒ CUDA Toolkit (nvcc) æœªå®‰è£…æˆ–ä¸åœ¨ PATH ä¸­\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140f4731",
   "metadata": {},
   "source": [
    "## 2. å®‰è£… Python 3.9 ğŸ\n",
    "\n",
    "å¦‚æœå½“å‰ Python ç‰ˆæœ¬ä¸æ˜¯ 3.9ï¼Œæˆ‘ä»¬éœ€è¦å®‰è£… Python 3.9 å¹¶è®¾ç½®è™šæ‹Ÿç¯å¢ƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc09395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ£€æŸ¥Pythonç‰ˆæœ¬å¹¶å®‰è£…Python 3.9\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "current_version = sys.version_info\n",
    "print(f\"å½“å‰ Python ç‰ˆæœ¬: {current_version.major}.{current_version.minor}.{current_version.micro}\")\n",
    "\n",
    "if current_version.major == 3 and current_version.minor == 9:\n",
    "    print(\"âœ… Python 3.9 å·²å®‰è£…ä¸”ä¸ºå½“å‰ç‰ˆæœ¬\")\n",
    "else:\n",
    "    print(\"âš ï¸  éœ€è¦å®‰è£… Python 3.9\")\n",
    "    print(\"\\næ‰§è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£… Python 3.9 (Ubuntu/Debian):\")\n",
    "    commands = [\n",
    "        \"sudo apt update\",\n",
    "        \"sudo apt install -y software-properties-common\",\n",
    "        \"sudo add-apt-repository -y ppa:deadsnakes/ppa\",\n",
    "        \"sudo apt update\", \n",
    "        \"sudo apt install -y python3.9 python3.9-dev python3.9-venv python3.9-distutils\",\n",
    "        \"sudo apt install -y python3-pip\",\n",
    "        \"python3.9 -m pip install --upgrade pip\"\n",
    "    ]\n",
    "    \n",
    "    for cmd in commands:\n",
    "        print(f\"$ {cmd}\")\n",
    "    \n",
    "    print(\"\\nâš ï¸  è¯·åœ¨ç»ˆç«¯ä¸­æ‰‹åŠ¨æ‰§è¡Œä¸Šè¿°å‘½ä»¤ï¼Œç„¶åé‡æ–°å¯åŠ¨æ­¤ç¬”è®°æœ¬\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489623db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºå’Œæ¿€æ´» Python 3.9 è™šæ‹Ÿç¯å¢ƒ\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "env_name = \"endomamba_env\"\n",
    "env_path = os.path.join(os.getcwd(), env_name)\n",
    "\n",
    "print(f\"ğŸ“ åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ: {env_name}\")\n",
    "print(f\"ğŸ“ ç¯å¢ƒè·¯å¾„: {env_path}\")\n",
    "\n",
    "try:\n",
    "    # æ£€æŸ¥è™šæ‹Ÿç¯å¢ƒæ˜¯å¦å·²å­˜åœ¨\n",
    "    if os.path.exists(env_path):\n",
    "        print(\"âœ… è™šæ‹Ÿç¯å¢ƒå·²å­˜åœ¨\")\n",
    "    else:\n",
    "        print(\"ğŸ”„ æ­£åœ¨åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ...\")\n",
    "        result = subprocess.run([\n",
    "            sys.executable, \"-m\", \"venv\", env_path\n",
    "        ], capture_output=True, text=True, check=True)\n",
    "        print(\"âœ… è™šæ‹Ÿç¯å¢ƒåˆ›å»ºæˆåŠŸ\")\n",
    "    \n",
    "    # æä¾›æ¿€æ´»å‘½ä»¤\n",
    "    if os.name == 'nt':  # Windows\n",
    "        activate_cmd = f\"{env_path}\\\\Scripts\\\\activate\"\n",
    "    else:  # Linux/Mac\n",
    "        activate_cmd = f\"source {env_path}/bin/activate\"\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ æ¿€æ´»è™šæ‹Ÿç¯å¢ƒå‘½ä»¤:\")\n",
    "    print(f\"$ {activate_cmd}\")\n",
    "    print(f\"\\nğŸ’¡ åœ¨è™šæ‹Ÿç¯å¢ƒä¸­è¿è¡Œ Jupyter:\")\n",
    "    print(f\"$ {activate_cmd}\")\n",
    "    print(f\"$ pip install jupyter\")\n",
    "    print(f\"$ jupyter notebook\")\n",
    "    \n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"âŒ åˆ›å»ºè™šæ‹Ÿç¯å¢ƒå¤±è´¥: {e}\")\n",
    "    print(\"è¯·ç¡®ä¿ Python 3.9 å·²æ­£ç¡®å®‰è£…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3602b35",
   "metadata": {},
   "source": [
    "## 3. å®‰è£… CUDA 12.4 âš¡\n",
    "\n",
    "å®‰è£… CUDA 12.4 å·¥å…·åŒ…ï¼Œé…ç½®ç¯å¢ƒå˜é‡ï¼ŒéªŒè¯ CUDA å®‰è£…å’Œ GPU å¯ç”¨æ€§ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0405380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA 12.4 å®‰è£…å‘½ä»¤\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "print(\"ğŸ”½ CUDA 12.4 å®‰è£…æ­¥éª¤ (Ubuntu/Linux)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# æ£€æŸ¥å½“å‰CUDAç‰ˆæœ¬\n",
    "try:\n",
    "    result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True, check=True)\n",
    "    print(\"âœ… å½“å‰CUDAçŠ¶æ€:\")\n",
    "    for line in result.stdout.split('\\n'):\n",
    "        if 'release' in line:\n",
    "            print(f\"å·²å®‰è£…ç‰ˆæœ¬: {line.strip()}\")\n",
    "            break\n",
    "except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "    print(\"âŒ CUDA Toolkit æœªå®‰è£…\")\n",
    "\n",
    "print(\"\\nğŸ“‹ å®‰è£… CUDA 12.4 çš„å‘½ä»¤:\")\n",
    "cuda_commands = [\n",
    "    \"# 1. ä¸‹è½½ CUDA 12.4 å®‰è£…åŒ…\",\n",
    "    \"wget https://developer.download.nvidia.com/compute/cuda/12.4.0/local_installers/cuda_12.4.0_550.54.14_linux.run\",\n",
    "    \"\",\n",
    "    \"# 2. ç»™å®‰è£…åŒ…æ‰§è¡Œæƒé™\",\n",
    "    \"chmod +x cuda_12.4.0_550.54.14_linux.run\",\n",
    "    \"\",\n",
    "    \"# 3. æ‰§è¡Œå®‰è£… (é™é»˜å®‰è£…ï¼Œè·³è¿‡é©±åŠ¨)\",\n",
    "    \"sudo sh cuda_12.4.0_550.54.14_linux.run --silent --toolkit\",\n",
    "    \"\",\n",
    "    \"# 4. æ·»åŠ ç¯å¢ƒå˜é‡åˆ° ~/.bashrc\",\n",
    "    'echo \\'export PATH=/usr/local/cuda-12.4/bin:$PATH\\' >> ~/.bashrc',\n",
    "    'echo \\'export LD_LIBRARY_PATH=/usr/local/cuda-12.4/lib64:$LD_LIBRARY_PATH\\' >> ~/.bashrc',\n",
    "    \"\",\n",
    "    \"# 5. é‡æ–°åŠ è½½ç¯å¢ƒå˜é‡\",\n",
    "    \"source ~/.bashrc\",\n",
    "    \"\",\n",
    "    \"# 6. éªŒè¯å®‰è£…\",\n",
    "    \"nvcc --version\"\n",
    "]\n",
    "\n",
    "for cmd in cuda_commands:\n",
    "    print(cmd)\n",
    "\n",
    "print(f\"\\nâš ï¸  é‡è¦æç¤º:\")\n",
    "print(\"1. è¯·ç¡®ä¿å·²å®‰è£…äº† NVIDIA é©±åŠ¨ (ç‰ˆæœ¬ >= 550.54.14)\")\n",
    "print(\"2. å¦‚æœé©±åŠ¨ç‰ˆæœ¬è¿‡ä½ï¼Œè¯·å…ˆæ›´æ–°é©±åŠ¨\")\n",
    "print(\"3. å®‰è£…å®Œæˆåéœ€è¦é‡å¯ç»ˆç«¯æˆ–é‡æ–°åŠ è½½ç¯å¢ƒå˜é‡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0c541e",
   "metadata": {},
   "source": [
    "## 4. å®‰è£… PyTorch 2.4.1+cu121 ğŸ”¥\n",
    "\n",
    "ä½¿ç”¨ pip å®‰è£… PyTorch 2.4.1+cu121 ç‰ˆæœ¬ï¼ŒåŒ…æ‹¬ torchvision å’Œ torchaudioï¼ŒéªŒè¯ CUDA æ”¯æŒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af7380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£… PyTorch 2.4.1+cu121\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"ğŸ”¥ å®‰è£… PyTorch 2.4.1+cu121\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# PyTorch å®‰è£…å‘½ä»¤\n",
    "pytorch_install_cmd = [\n",
    "    sys.executable, \"-m\", \"pip\", \"install\",\n",
    "    \"--extra-index-url\", \"https://download.pytorch.org/whl/cu121\",\n",
    "    \"torch==2.4.1+cu121\",\n",
    "    \"torchvision==0.19.1+cu121\", \n",
    "    \"torchaudio==2.4.1+cu121\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ“¦ æ­£åœ¨å®‰è£… PyTorch, torchvision, torchaudio...\")\n",
    "print(\"å‘½ä»¤:\", \" \".join(pytorch_install_cmd))\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(pytorch_install_cmd, capture_output=True, text=True, check=True)\n",
    "    print(\"âœ… PyTorch å®‰è£…æˆåŠŸ!\")\n",
    "    if result.stdout:\n",
    "        print(\"å®‰è£…æ—¥å¿—:\")\n",
    "        print(result.stdout[-500:])  # æ˜¾ç¤ºæœ€å500ä¸ªå­—ç¬¦\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"âŒ PyTorch å®‰è£…å¤±è´¥: {e}\")\n",
    "    print(\"é”™è¯¯ä¿¡æ¯:\")\n",
    "    print(e.stderr)\n",
    "    \n",
    "print(\"\\nğŸ” éªŒè¯ PyTorch å®‰è£…...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63944dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# éªŒè¯ PyTorch å®‰è£…å’Œ CUDA æ”¯æŒ\n",
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    import torchaudio\n",
    "    \n",
    "    print(\"âœ… PyTorch å¯¼å…¥æˆåŠŸ!\")\n",
    "    print(f\"PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
    "    print(f\"TorchVision ç‰ˆæœ¬: {torchvision.__version__}\")\n",
    "    print(f\"TorchAudio ç‰ˆæœ¬: {torchaudio.__version__}\")\n",
    "    \n",
    "    print(f\"\\nğŸ® CUDA æ”¯æŒ:\")\n",
    "    print(f\"CUDA å¯ç”¨: {'âœ… æ˜¯' if torch.cuda.is_available() else 'âŒ å¦'}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA ç‰ˆæœ¬: {torch.version.cuda}\")\n",
    "        print(f\"cuDNN ç‰ˆæœ¬: {torch.backends.cudnn.version()}\")\n",
    "        print(f\"cuDNN å¯ç”¨: {'âœ… æ˜¯' if torch.backends.cudnn.enabled else 'âŒ å¦'}\")\n",
    "        print(f\"GPU è®¾å¤‡æ•°é‡: {torch.cuda.device_count()}\")\n",
    "        \n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "            \n",
    "        # æµ‹è¯•ç®€å•çš„CUDAæ“ä½œ\n",
    "        print(f\"\\nğŸ§ª CUDA åŠŸèƒ½æµ‹è¯•:\")\n",
    "        x = torch.randn(3, 3).cuda()\n",
    "        y = torch.randn(3, 3).cuda()\n",
    "        z = x + y\n",
    "        print(f\"CUDA å¼ é‡è¿ç®—æµ‹è¯•: {'âœ… é€šè¿‡' if z.is_cuda else 'âŒ å¤±è´¥'}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âš ï¸  CUDA ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥:\")\n",
    "        print(\"1. NVIDIA é©±åŠ¨æ˜¯å¦æ­£ç¡®å®‰è£…\")\n",
    "        print(\"2. CUDA Toolkit æ˜¯å¦å®‰è£…\")\n",
    "        print(\"3. PyTorch æ˜¯å¦ä¸º CUDA ç‰ˆæœ¬\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"âŒ PyTorch å¯¼å…¥å¤±è´¥: {e}\")\n",
    "    print(\"è¯·æ£€æŸ¥ PyTorch æ˜¯å¦æ­£ç¡®å®‰è£…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487ef34b",
   "metadata": {},
   "source": [
    "## 5. å®‰è£…å…¶ä»–ä¾èµ–é¡¹ ğŸ“¦\n",
    "\n",
    "æ‰¹é‡å®‰è£… requirements.txt ä¸­çš„ä¾èµ–é¡¹ï¼ŒåŒ…æ‹¬æ·±åº¦å­¦ä¹ æ¡†æ¶ã€è®¡ç®—æœºè§†è§‰åº“ã€æ•°æ®ç§‘å­¦å·¥å…·ç­‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ff6e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704fd577",
   "metadata": {},
   "source": [
    "## 6. ä»æºä»£ç æ„å»ºå®‰è£… causal-conv1d ğŸ”¨\n",
    "\n",
    "å…‹éš† causal-conv1d æºä»£ç ä»“åº“ï¼Œç¼–è¯‘å¹¶å®‰è£…ï¼Œå¤„ç†å¯èƒ½çš„æ„å»ºé”™è¯¯å’Œä¾èµ–é—®é¢˜ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71d7e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ./videomamba/causal-conv1d --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b486745d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯• causal-conv1d åŸºæœ¬åŠŸèƒ½ ğŸ§ª\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"ğŸ§ª æµ‹è¯• causal-conv1d åŸºæœ¬åŠŸèƒ½\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    from causal_conv1d import causal_conv1d_fn\n",
    "    \n",
    "    # è®¾ç½®æµ‹è¯•å‚æ•°\n",
    "    batch_size = 2\n",
    "    seq_len = 128\n",
    "    dim = 64\n",
    "    width = 4\n",
    "    \n",
    "    print(f\"ğŸ“‹ æµ‹è¯•å‚æ•°:\")\n",
    "    print(f\"  æ‰¹æ¬¡å¤§å°: {batch_size}\")\n",
    "    print(f\"  åºåˆ—é•¿åº¦: {seq_len}\")\n",
    "    print(f\"  ç‰¹å¾ç»´åº¦: {dim}\")\n",
    "    print(f\"  å·ç§¯å®½åº¦: {width}\")\n",
    "    \n",
    "    # åˆ›å»ºæµ‹è¯•æ•°æ®\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"ğŸ® ä½¿ç”¨è®¾å¤‡: {device}\")\n",
    "    \n",
    "    # è¾“å…¥å¼ é‡ (batch, dim, seq_len)\n",
    "    x = torch.randn(batch_size, dim, seq_len, device=device, dtype=torch.float32)\n",
    "    \n",
    "    # æƒé‡å¼ é‡ (dim, width)\n",
    "    weight = torch.randn(dim, width, device=device, dtype=torch.float32)\n",
    "    \n",
    "    # åç½®å¼ é‡ (dim,)\n",
    "    bias = torch.randn(dim, device=device, dtype=torch.float32)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š å¼ é‡å½¢çŠ¶:\")\n",
    "    print(f\"  è¾“å…¥ x: {x.shape}\")\n",
    "    print(f\"  æƒé‡ weight: {weight.shape}\")\n",
    "    print(f\"  åç½® bias: {bias.shape}\")\n",
    "    \n",
    "    # æ‰§è¡Œ causal conv1d æ“ä½œ\n",
    "    print(f\"\\nğŸ”„ æ‰§è¡Œ causal conv1d è¿ç®—...\")\n",
    "    \n",
    "    # æµ‹è¯•ä¸åŒçš„æ¿€æ´»å‡½æ•°\n",
    "    activations = ['silu', 'swish', None]\n",
    "    \n",
    "    for activation in activations:\n",
    "        try:\n",
    "            if activation:\n",
    "                output = causal_conv1d_fn(x, weight, bias, activation=activation)\n",
    "                print(f\"âœ… {activation} æ¿€æ´»å‡½æ•°æµ‹è¯•é€šè¿‡ï¼Œè¾“å‡ºå½¢çŠ¶: {output.shape}\")\n",
    "            else:\n",
    "                output = causal_conv1d_fn(x, weight, bias)\n",
    "                print(f\"âœ… æ— æ¿€æ´»å‡½æ•°æµ‹è¯•é€šè¿‡ï¼Œè¾“å‡ºå½¢çŠ¶: {output.shape}\")\n",
    "                \n",
    "            # éªŒè¯è¾“å‡ºçš„åˆç†æ€§\n",
    "            if output.shape == x.shape:\n",
    "                print(f\"  âœ“ è¾“å‡ºå½¢çŠ¶æ­£ç¡®\")\n",
    "            else:\n",
    "                print(f\"  âŒ è¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…ï¼ŒæœŸæœ›: {x.shape}, å®é™…: {output.shape}\")\n",
    "                \n",
    "            # æ£€æŸ¥è¾“å‡ºæ˜¯å¦åŒ…å«æœ‰æ•ˆæ•°å€¼\n",
    "            if torch.isfinite(output).all():\n",
    "                print(f\"  âœ“ è¾“å‡ºæ•°å€¼æœ‰æ•ˆ\")\n",
    "            else:\n",
    "                print(f\"  âŒ è¾“å‡ºåŒ…å«æ— æ•ˆæ•°å€¼ (NaN/Inf)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ {activation or 'æ— æ¿€æ´»'} æµ‹è¯•å¤±è´¥: {e}\")\n",
    "    \n",
    "    # æ€§èƒ½æµ‹è¯•\n",
    "    print(f\"\\nâš¡ æ€§èƒ½æµ‹è¯•:\")\n",
    "    \n",
    "    # é¢„çƒ­\n",
    "    for _ in range(5):\n",
    "        _ = causal_conv1d_fn(x, weight, bias)\n",
    "    \n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # è®¡æ—¶\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    num_iterations = 100\n",
    "    \n",
    "    for _ in range(num_iterations):\n",
    "        output = causal_conv1d_fn(x, weight, bias)\n",
    "    \n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    avg_time = (end_time - start_time) / num_iterations * 1000  # æ¯«ç§’\n",
    "    \n",
    "    print(f\"  å¹³å‡æ‰§è¡Œæ—¶é—´: {avg_time:.2f} ms\")\n",
    "    print(f\"  ååé‡: {batch_size * seq_len / (avg_time / 1000):.0f} tokens/sec\")\n",
    "    \n",
    "    # å†…å­˜ä½¿ç”¨æµ‹è¯•\n",
    "    if device.type == 'cuda':\n",
    "        memory_before = torch.cuda.memory_allocated(device) / 1024**2\n",
    "        \n",
    "        # åˆ›å»ºè¾ƒå¤§çš„å¼ é‡è¿›è¡Œæµ‹è¯•\n",
    "        large_x = torch.randn(8, 512, 2048, device=device, dtype=torch.float32)\n",
    "        large_weight = torch.randn(512, 4, device=device, dtype=torch.float32)\n",
    "        large_bias = torch.randn(512, device=device, dtype=torch.float32)\n",
    "        \n",
    "        large_output = causal_conv1d_fn(large_x, large_weight, large_bias)\n",
    "        \n",
    "        memory_after = torch.cuda.memory_allocated(device) / 1024**2\n",
    "        memory_used = memory_after - memory_before\n",
    "        \n",
    "        print(f\"\\nğŸ’¾ å†…å­˜ä½¿ç”¨æµ‹è¯•:\")\n",
    "        print(f\"  æµ‹è¯•å¼ é‡å¤§å°: {large_x.shape}\")\n",
    "        print(f\"  å†…å­˜ä½¿ç”¨é‡: {memory_used:.1f} MB\")\n",
    "        \n",
    "        # æ¸…ç†å†…å­˜\n",
    "        del large_x, large_weight, large_bias, large_output\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\nğŸ‰ causal-conv1d åŠŸèƒ½æµ‹è¯•å…¨éƒ¨é€šè¿‡!\")\n",
    "    print(f\"âœ… åº“å®‰è£…æ­£ç¡®ï¼Œå¯ä»¥æ­£å¸¸ä½¿ç”¨\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ causal-conv1d åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}\")\n",
    "    print(f\"ğŸ’¡ å¯èƒ½çš„é—®é¢˜:\")\n",
    "    print(f\"1. CUDA ç¯å¢ƒé…ç½®ä¸æ­£ç¡®\")\n",
    "    print(f\"2. PyTorch ç‰ˆæœ¬ä¸å…¼å®¹\")\n",
    "    print(f\"3. ç¼–è¯‘æ—¶å‡ºç°é—®é¢˜\")\n",
    "    \n",
    "    # æä¾›è°ƒè¯•ä¿¡æ¯\n",
    "    print(f\"\\nğŸ” è°ƒè¯•ä¿¡æ¯:\")\n",
    "    try:\n",
    "        import causal_conv1d\n",
    "        print(f\"causal_conv1d ç‰ˆæœ¬: {getattr(causal_conv1d, '__version__', 'æœªçŸ¥')}\")\n",
    "        print(f\"causal_conv1d è·¯å¾„: {causal_conv1d.__file__}\")\n",
    "    except:\n",
    "        print(\"æ— æ³•è·å– causal_conv1d è°ƒè¯•ä¿¡æ¯\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2d2287",
   "metadata": {},
   "source": [
    "## 7. ä»æºä»£ç æ„å»ºå®‰è£…è‡ªå®šä¹‰ Mamba ğŸ\n",
    "\n",
    "ä¸‹è½½å¹¶ç¼–è¯‘å®‰è£… Mamba SSM åº“ï¼Œé…ç½®è‡ªå®šä¹‰ç‰ˆæœ¬ï¼Œç¡®ä¿ä¸é¡¹ç›®å…¼å®¹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b53adac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ./videomamba/_mamba\n",
    "export MAMBA_FORCE_BUILD=TRUE\n",
    "python setup.py clean\n",
    "rm -rf build/ *.egg-info\n",
    "pip install . --no-build-isolation --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3aaf21",
   "metadata": {},
   "source": [
    "## 8. å®‰è£…éªŒè¯å’Œå¿«é€Ÿæ¼”ç¤ºæµ‹è¯• âœ…\n",
    "\n",
    "è¿è¡Œ endomamba_demo.py æ¼”ç¤ºè„šæœ¬ï¼ŒéªŒè¯æ‰€æœ‰ç»„ä»¶æ­£å¸¸å·¥ä½œï¼Œæµ‹è¯•æ¨¡å‹åŠ è½½å’Œæ¨ç†åŠŸèƒ½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "580c4979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” å®Œæ•´ç¯å¢ƒéªŒè¯\n",
      "============================================================\n",
      "ğŸ“¦ éªŒè¯å…³é”®åº“å®‰è£…çŠ¶æ€:\n",
      "----------------------------------------\n",
      "âŒ PyTorch: æœªå®‰è£…\n",
      "âŒ TorchVision: æœªå®‰è£…\n",
      "âŒ TorchAudio: æœªå®‰è£…\n",
      "âŒ NumPy: æœªå®‰è£…\n",
      "âŒ OpenCV: æœªå®‰è£…\n",
      "âŒ Pillow: æœªå®‰è£…\n",
      "âŒ Timm: æœªå®‰è£…\n",
      "âŒ Einops: æœªå®‰è£…\n",
      "âŒ Transformers: æœªå®‰è£…\n",
      "âŒ Weights & Biases: æœªå®‰è£…\n",
      "âŒ Scikit-learn: æœªå®‰è£…\n",
      "âŒ Pandas: æœªå®‰è£…\n",
      "âŒ Matplotlib: æœªå®‰è£…\n",
      "âœ… TQDM: 4.66.2\n",
      "âœ… PyYAML: 6.0.2\n",
      "âŒ Causal Conv1D: æœªå®‰è£…\n",
      "âŒ Mamba SSM: æœªå®‰è£…\n",
      "\n",
      "ğŸ“Š å®‰è£…ç»Ÿè®¡: 2/17 (11.8%)\n",
      "\n",
      "ğŸ® CUDA åŠŸèƒ½æµ‹è¯•:\n",
      "----------------------------------------\n",
      "âŒ CUDA æµ‹è¯•å¤±è´¥: No module named 'torch'\n",
      "\n",
      "âš ï¸  ç¯å¢ƒéªŒè¯ä¸å®Œæ•´ï¼Œè¯·æ£€æŸ¥å¤±è´¥çš„åº“å®‰è£…\n"
     ]
    }
   ],
   "source": [
    "# å®Œæ•´ç¯å¢ƒéªŒè¯\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "print(\"ğŸ” å®Œæ•´ç¯å¢ƒéªŒè¯\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# å…³é”®åº“éªŒè¯åˆ—è¡¨\n",
    "key_libraries = [\n",
    "    (\"torch\", \"PyTorch\"),\n",
    "    (\"torchvision\", \"TorchVision\"), \n",
    "    (\"torchaudio\", \"TorchAudio\"),\n",
    "    (\"numpy\", \"NumPy\"),\n",
    "    (\"cv2\", \"OpenCV\"),\n",
    "    (\"PIL\", \"Pillow\"),\n",
    "    (\"timm\", \"Timm\"),\n",
    "    (\"einops\", \"Einops\"),\n",
    "    (\"transformers\", \"Transformers\"),\n",
    "    (\"wandb\", \"Weights & Biases\"),\n",
    "    (\"sklearn\", \"Scikit-learn\"),\n",
    "    (\"pandas\", \"Pandas\"),\n",
    "    (\"matplotlib\", \"Matplotlib\"),\n",
    "    (\"tqdm\", \"TQDM\"),\n",
    "    (\"yaml\", \"PyYAML\"),\n",
    "    (\"causal_conv1d\", \"Causal Conv1D\"),\n",
    "    (\"mamba_ssm\", \"Mamba SSM\"),\n",
    "]\n",
    "\n",
    "print(\"ğŸ“¦ éªŒè¯å…³é”®åº“å®‰è£…çŠ¶æ€:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "success_count = 0\n",
    "total_count = len(key_libraries)\n",
    "\n",
    "for module_name, display_name in key_libraries:\n",
    "    try:\n",
    "        module = importlib.import_module(module_name)\n",
    "        version = getattr(module, '__version__', 'æœªçŸ¥ç‰ˆæœ¬')\n",
    "        print(f\"âœ… {display_name}: {version}\")\n",
    "        success_count += 1\n",
    "    except ImportError:\n",
    "        print(f\"âŒ {display_name}: æœªå®‰è£…\")\n",
    "\n",
    "print(f\"\\nğŸ“Š å®‰è£…ç»Ÿè®¡: {success_count}/{total_count} ({success_count/total_count*100:.1f}%)\")\n",
    "\n",
    "# CUDA åŠŸèƒ½æµ‹è¯•\n",
    "print(f\"\\nğŸ® CUDA åŠŸèƒ½æµ‹è¯•:\")\n",
    "print(\"-\" * 40)\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print(f\"âœ… CUDA è®¾å¤‡: {torch.cuda.get_device_name()}\")\n",
    "        \n",
    "        # åˆ›å»ºæµ‹è¯•å¼ é‡å¹¶è¿›è¡Œè®¡ç®—\n",
    "        x = torch.randn(1000, 1000, device=device)\n",
    "        y = torch.randn(1000, 1000, device=device)\n",
    "        z = torch.mm(x, y)\n",
    "        print(f\"âœ… CUDA çŸ©é˜µè¿ç®—æµ‹è¯•é€šè¿‡\")\n",
    "        \n",
    "        # æµ‹è¯•å†…å­˜\n",
    "        memory_allocated = torch.cuda.memory_allocated(device) / 1024**2\n",
    "        memory_cached = torch.cuda.memory_reserved(device) / 1024**2\n",
    "        print(f\"ğŸ“Š GPU å†…å­˜ä½¿ç”¨: {memory_allocated:.1f}MB å·²åˆ†é…, {memory_cached:.1f}MB å·²ç¼“å­˜\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ CUDA ä¸å¯ç”¨\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ CUDA æµ‹è¯•å¤±è´¥: {e}\")\n",
    "\n",
    "if success_count >= total_count * 0.8:  # 80%æˆåŠŸç‡\n",
    "    print(f\"\\nğŸ‰ ç¯å¢ƒéªŒè¯é€šè¿‡! å¯ä»¥ç»§ç»­æ¼”ç¤ºæµ‹è¯•\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  ç¯å¢ƒéªŒè¯ä¸å®Œæ•´ï¼Œè¯·æ£€æŸ¥å¤±è´¥çš„åº“å®‰è£…\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a58cf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿è¡Œ EndoMamba æ¼”ç¤ºè„šæœ¬\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"ğŸš€ è¿è¡Œ EndoMamba æ¼”ç¤ºæµ‹è¯•\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# æ£€æŸ¥æ¼”ç¤ºè„šæœ¬è·¯å¾„\n",
    "demo_script_path = \"videomamba/tests/endomamba_demo.py\"\n",
    "\n",
    "if os.path.exists(demo_script_path):\n",
    "    print(f\"âœ… æ‰¾åˆ°æ¼”ç¤ºè„šæœ¬: {demo_script_path}\")\n",
    "    \n",
    "    try:\n",
    "        print(\"ğŸ”„ æ­£åœ¨è¿è¡Œæ¼”ç¤ºè„šæœ¬...\")\n",
    "        print(\"âš ï¸  æ³¨æ„: é¦–æ¬¡è¿è¡Œå¯èƒ½éœ€è¦ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¯·è€å¿ƒç­‰å¾…\")\n",
    "        \n",
    "        # åˆ‡æ¢åˆ°æ­£ç¡®çš„ç›®å½•\n",
    "        original_dir = os.getcwd()\n",
    "        test_dir = \"videomamba/tests\"\n",
    "        \n",
    "        os.chdir(test_dir)\n",
    "        print(f\"ğŸ“ å½“å‰ç›®å½•: {os.getcwd()}\")\n",
    "        \n",
    "        # è¿è¡Œæ¼”ç¤ºè„šæœ¬\n",
    "        result = subprocess.run([\n",
    "            sys.executable, \"endomamba_demo.py\"\n",
    "        ], capture_output=True, text=True, timeout=300, check=True)  # 5åˆ†é’Ÿè¶…æ—¶\n",
    "        \n",
    "        print(\"âœ… æ¼”ç¤ºè„šæœ¬æ‰§è¡ŒæˆåŠŸ!\")\n",
    "        print(\"\\nğŸ“‹ è¾“å‡ºç»“æœ:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(result.stdout)\n",
    "        \n",
    "        if result.stderr:\n",
    "            print(\"\\nâš ï¸  è­¦å‘Šä¿¡æ¯:\")\n",
    "            print(result.stderr)\n",
    "            \n",
    "        os.chdir(original_dir)\n",
    "        \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"â° æ¼”ç¤ºè„šæœ¬è¿è¡Œè¶…æ—¶ (5åˆ†é’Ÿ)\")\n",
    "        print(\"ğŸ’¡ è¿™å¯èƒ½æ˜¯ç”±äºæ¨¡å‹ä¸‹è½½æˆ–è®¡ç®—æ—¶é—´è¿‡é•¿\")\n",
    "        os.chdir(original_dir)\n",
    "        \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"âŒ æ¼”ç¤ºè„šæœ¬æ‰§è¡Œå¤±è´¥: {e}\")\n",
    "        print(\"\\né”™è¯¯è¾“å‡º:\")\n",
    "        print(e.stderr)\n",
    "        print(\"\\nğŸ’¡ å¯èƒ½çš„é—®é¢˜:\")\n",
    "        print(\"1. ç¼ºå°‘é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶\")\n",
    "        print(\"2. GPU å†…å­˜ä¸è¶³\")\n",
    "        print(\"3. æŸäº›ä¾èµ–åº“æœªæ­£ç¡®å®‰è£…\")\n",
    "        os.chdir(original_dir)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è¿è¡Œæ¼”ç¤ºæ—¶å‘ç”Ÿé”™è¯¯: {e}\")\n",
    "        os.chdir(original_dir)\n",
    "        \n",
    "else:\n",
    "    print(f\"âŒ æœªæ‰¾åˆ°æ¼”ç¤ºè„šæœ¬: {demo_script_path}\")\n",
    "    print(\"ğŸ’¡ è¯·ç¡®ä¿åœ¨ EndoMamba é¡¹ç›®æ ¹ç›®å½•è¿è¡Œæ­¤è„šæœ¬\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9189486",
   "metadata": {},
   "source": [
    "## ğŸ‰ å®‰è£…å®Œæˆæ€»ç»“\n",
    "\n",
    "æ­å–œï¼æ‚¨å·²ç»å®Œæˆäº† EndoMamba é¡¹ç›®çš„å®Œæ•´ç¯å¢ƒé…ç½®ã€‚\n",
    "\n",
    "### âœ… å·²å®Œæˆçš„æ­¥éª¤:\n",
    "\n",
    "1. **ç¯å¢ƒæ£€æµ‹** - æ£€æŸ¥ç³»ç»Ÿé…ç½®å’Œç¡¬ä»¶ä¿¡æ¯\n",
    "2. **Python 3.9** - å®‰è£…å’Œé…ç½® Python 3.9 ç¯å¢ƒ  \n",
    "3. **CUDA 12.4** - å®‰è£… CUDA å·¥å…·åŒ…å’Œé©±åŠ¨\n",
    "4. **PyTorch 2.4.1+cu121** - å®‰è£… GPU æ”¯æŒçš„ PyTorch\n",
    "5. **ä¾èµ–é¡¹å®‰è£…** - æ‰¹é‡å®‰è£…æ‰€æœ‰å¿…éœ€çš„ Python åº“\n",
    "6. **Causal-Conv1D** - ä»æºä»£ç æ„å»ºè‡ªå®šä¹‰å·ç§¯åº“\n",
    "7. **Mamba SSM** - å®‰è£…çŠ¶æ€ç©ºé—´æ¨¡å‹åº“\n",
    "8. **åŠŸèƒ½éªŒè¯** - è¿è¡Œæ¼”ç¤ºè„šæœ¬éªŒè¯ç³»ç»Ÿ\n",
    "\n",
    "### ğŸš€ åç»­ä½¿ç”¨æŒ‡å—:\n",
    "\n",
    "#### è®­ç»ƒæ¨¡å‹\n",
    "```bash\n",
    "cd videomamba/video_sm\n",
    "python run_endomamba_pretraining.py --config configs/your_config.yaml\n",
    "```\n",
    "\n",
    "#### å¾®è°ƒæ¨¡å‹  \n",
    "```bash\n",
    "python run_class_finetuning.py --model endomamba_small --dataset your_dataset\n",
    "```\n",
    "\n",
    "#### æ¨ç†é¢„æµ‹\n",
    "```bash\n",
    "python run_inference.py --model_path /path/to/model --input_video /path/to/video\n",
    "```\n",
    "\n",
    "### ğŸ“š é‡è¦æ–‡æ¡£:\n",
    "- é¡¹ç›®æ–‡æ¡£: `README.md`\n",
    "- æ•°æ®é›†é…ç½®: `videomamba/video_sm/DATASET.md`\n",
    "- æ¨¡å‹é…ç½®: `videomamba/video_sm/models/`\n",
    "\n",
    "### âš ï¸  å¸¸è§é—®é¢˜:\n",
    "1. **GPU å†…å­˜ä¸è¶³**: å‡å°‘ batch_size æˆ–ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯\n",
    "2. **æ¨¡å‹ä¸‹è½½æ…¢**: ä½¿ç”¨å›½å†…é•œåƒæˆ–æ‰‹åŠ¨ä¸‹è½½\n",
    "3. **ç¼–è¯‘é”™è¯¯**: ç¡®ä¿ CUDA å’Œ gcc ç‰ˆæœ¬å…¼å®¹\n",
    "\n",
    "### ğŸ“ æŠ€æœ¯æ”¯æŒ:\n",
    "å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥:\n",
    "- CUDA å’Œ PyTorch ç‰ˆæœ¬å…¼å®¹æ€§\n",
    "- GPU é©±åŠ¨æ˜¯å¦æœ€æ–°\n",
    "- æ‰€æœ‰ä¾èµ–é¡¹æ˜¯å¦æ­£ç¡®å®‰è£…\n",
    "\n",
    "**ğŸŠ ç¯å¢ƒé…ç½®å®Œæˆï¼Œå¼€å§‹æ‚¨çš„ EndoMamba ä¹‹æ—…ï¼**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
